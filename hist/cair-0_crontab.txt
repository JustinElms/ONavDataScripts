# Notice that tasks will be started based on the cron's system
# daemon's notion of time and timezones.
#
# Output of the crontab jobs (including errors) is sent through
# email to the user the crontab file belongs to (unless redirected).
#
# For example, you can run a backup of all your user accounts
# at 5 a.m every week with:
# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/
#
# For more information see the manual pages of crontab(5) and cron(8)
#
# m h  dom mon dow   command

### The one-off-model script copies any missing day from the last 10 days for a given
### model
05 04,22 * * * ${HOME}/onav-cloud/bin/one-off-model.sh 10 model_ciops ; ${HOME}/onav-cloud/bin/one-off-model.sh 10 model_giops ; ${HOME}/onav-cloud/bin/one-off-model.sh 10 model_gdwps ; ${HOME}/onav-cloud/bin/one-off-model.sh 10 model_riops ; ${HOME}/onav-cloud/bin/one-off-model.sh 10 model_wcps

### The various sync-<model> script brings in todayâ€™s dataset from
### https://hpfx.collab.science.gc.ca/
### The loan indexing script for CIOPS East, West, and Salish Sea could easily have been
### configured to run using the INDEX LXC container
01 */6 * * * ${HOME}/onav-cloud/bin/sync-giops.sh
01 */6 * * * ${HOME}/onav-cloud/bin/sync-ciops.sh ; ${HOME}/onav-cloud/bin/index-ciops-east.sh ; ${HOME}/onav-cloud/bin/index-ciops-salish.sh ; ${HOME}/onav-cloud/bin/index-ciops-west.sh
01 */6 * * * ${HOME}/onav-cloud/bin/sync-gdwps.sh
01 */6 * * * ${HOME}/onav-cloud/bin/sync-riops.sh
01 */6 * * * ${HOME}/onav-cloud/bin/sync-wcps.sh
#01 */6 * * * ${HOME}/onav-cloud/bin/sync-port_model_verification.sh

### This script will create symbolic links from YYYY/YYYYMMDD to YYYY as the original pickle
### process did not know how to deal with files stored in multiple subdirectories.
05 */12 * * * ${HOME}/bin/create-class4-ln.sh

### Create a screen session, log into the u2004-index LXC container to source the environment
### configuration file, activate the index-tool Miniconda toolchain, and then launch the given
### script to create new or modify existing SQLite3 formatted databases to contain file location,
### various variables, and time.
40 02,14 * * * screen -A -m -d -S U2004-INDEX-GIOPS-DAILY ssh ubuntu@u2004-index "source tools/conf/ocean-navigator-env.sh ; conda activate index-tool ; cd index-scripts ; ./giops-daily.sh"
40 02,14 * * * screen -A -m -d -S U2004-INDEX-GIOPS-10DAY ssh ubuntu@u2004-index "source tools/conf/ocean-navigator-env.sh ; conda activate index-tool ; cd index-scripts ; ./giops-10day.sh"
40 02,14 * * * screen -A -m -d -S U2004-INDEX-RIOPS ssh ubuntu@u2004-index "source tools/conf/ocean-navigator-env.sh ; conda activate index-tool ; cd index-scripts ; ./riops-hourly.sh"
40 02,14 * * * screen -A -m -d -S U2004-INDEX-WCPS ssh ubuntu@u2004-index "source tools/conf/ocean-navigator-env.sh ; conda activate index-tool ; cd index-scripts ; ./wcps-2023.sh"

### Remove the ${HOME}/cache/oceannavigator/api/v1.0/tiles/[a-z]* directories in order to
### flush the server caching information.
05 */6 * * * lxc exec lxc-u2204-prod-0 -- su - ubuntu -c "rm -r cache/oceannavigator/{api,data}/"
05 */6 * * * lxc exec lxc-u2204-prod-1 -- su - ubuntu -c "rm -r cache/oceannavigator/{api,data}/"
05 */6 * * * lxc exec lxc-u2204-prod-2 -- su - ubuntu -c "rm -r cache/oceannavigator/{api,data}/"

### Create the pickle files for both the traditional CLASS4 and CLASS4 OLA datasets that are
### provided by SeDOO.
15 */6 * * * lxc exec lxc-u2204-prod-0 -- su - ubuntu -c "source onav-cloud/etc/ocean-navigator-env.sh ; ./onav-cloud/Ocean-Data-Map-Project/bin/launch-pickle.sh"
15 */6 * * * lxc exec lxc-u2204-prod-1 -- su - ubuntu -c "source onav-cloud/etc/ocean-navigator-env.sh ; ./onav-cloud/Ocean-Data-Map-Project/bin/launch-pickle.sh"
15 */6 * * * lxc exec lxc-u2204-prod-2 -- su - ubuntu -c "source onav-cloud/etc/ocean-navigator-env.sh ; ./onav-cloud/Ocean-Data-Map-Project/bin/launch-pickle.sh"

### Renew SSL certs on Fridays at 10:30 NST
#0 13 * * 5 sudo /usr/bin/certbot renew --quiet
